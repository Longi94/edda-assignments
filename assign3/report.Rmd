---
title: "Assignment 3"
author: "Daniel Galea, Konrad Karas and Long Tran, group 6"
date: "14 March 2018"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
library(multcomp)
library(lme4)
```

## Exercises

### Exercise 1

**Point 1**

**Point 2**

**Point 3**

**Point 4**

**Point 5**

### Exercise 2

```{r}
search = read.table("search.txt", header = TRUE)
attach(search)
```

**Point 1**

We can randomly assign each student to an interface using the following code.

```{r}
B = 5 # 5 user skill levels
I = 3 # 3 interface types

blocks = matrix(0, B, I)

for (i in 1:B) {
  blocks[i, ] = sample(((i - 1) * I + 1):(i * I))
}

kable(x = blocks, caption = "Randomization of students to interfaces")
```

The numbers in the table are the numbers assigned to each student and each row represents the type of student. The first row contains the student with skill level 1, the second row with skill level 2 and so on. In each row the first student will be assigned to interface 1, the second to interface 2 and the third to interface 3.

**Point 2**

We plot boxplots and interaction graphs.

```{r}
par(mfrow = c(1, 2))

boxplot(time ~ interface)
boxplot(time ~ skill)

interaction.plot(interface, skill, time)
interaction.plot(skill, interface, time)
```

On the first interaction graph a lot of lines are parallel, whil one the second one a lot of lies are nonparallel. We observe more parallel lines than nonparallel ones so we can conclude that there is no significatnt interaction between skill and interface.

**Point 3**

Since the levels of the factors are coded in numbers, we convert them to factors first.

```{r}
search$interface = as.factor(interface)
search$skill = as.factor(skill)
```

We do a 2-way ANOVA test to test the null hypothesis that the search time is the same for all three interfaces. Since in Point 2 we concluded that there are no interaction between skill and interface we do the test on the additive model.

```{r}
aovser = lm(time ~ interface + skill, data = search)
anova(aovser)
```

Both p-values are lower than 0.05, which means that both factors have a significant affect on the model and the null hypothesis can be rejected. The search time is not the same for all interfaces.

**Point 4**

```{r}
multser = glht(aovser, linfct = mcp(skill = "Tukey"))
summary(multser)
```

**Point 5**

```{r}
par(mfrow = c(1, 2))
qqnorm(residuals(aovser))
qqline(residuals(aovser))
plot(fitted(aovser), residuals(aovser))
```

The Q-Q plot indicates a normal distribution and the residuals are distributed evenly. The ANOVA test assumed a normal distribution so these diagnostic plots back the results of the test.

**Point 6**

```{r}
friedman.test(time, interface, skill)
```

We got a p-value of 0.04076 so the null hypothesis that the interface has no effect on the search time can be rejected.

**Point 7**

We do a 1-way ANOVA test.

```{r}
aovinter = lm(time ~ interface, data = search)
anova(aovinter)
```

We got a p-value of 0.09642 so we cannot reject the null hypothesis, that the search time is the same for all interfaces. In the interaction graphs in Point 2 we obsorve that skill as a high impact on the outcome measurement. This analysis is not right to perform since we are ignoring the `skill` variable.

The ANOVA test assumes normal distribution and we can verify this with a Q-Q plot. THe plot below maybe curving slightly, but base don this plot it is still reasonable to assume normal distribution.

```{r}
qqnorm(residuals(aovinter))
qqline(residuals(aovinter))
```

### Exercise 3
```{r}
data = read.table("cream.txt", header=TRUE)
data$starter = factor(data$starter)
data$batch = factor(data$batch)
data$position = factor(data$position)
attach(data)
```


**Point 1**


A three-way test was performed in the following way:
```{r}
model1 = lm(acidity~starter+batch+position,data)
model2 = update(model1, . ~ . - starter:batch:position) #remove interactions
data.aov = aov(model2,data)
summary(data.aov)
```


//Not sure what to write about the summary yet.
**Point 2**


In order to test the hypotheses for all $$H0:\alpha i = \alpha i'$$ on equality of differences of the main effects for starter simultaneously, a multiple comparison test was performed.


```{r}
model = lm(acidity~starter+batch+position, data=data)
startermult = glht(model,linfct=mcp(starter="Tukey"))
summary(startermult)
```


Looking at the P values produced, we can see that $$ \alpha 4 = \alpha 1 $$,$$ \alpha 4 = \alpha 2 $$, $$ \alpha 4 = \alpha 3 $$, and $$ \alpha 5 = \alpha 4 $$ for the previously mentioned null hypothesis are rejected as they are  < 0.05


**Point 3**
//Not sure about this part. can't fid P value from point 1 summary that matches alpha2 = alpha1

**Point 4**
In order to find the confidence intervals for testing all differences of the main effect of starter we could use the "startermult" variable from the second question of this exercise. The test was performed in the following way:


```{r}
confint(startermult, level=0.95)
```
Looking at the result, we can conclude that the intervals [1.3201,4.2999], [1.4701,4.4499], [2.3001,5.2799], and [-4.7835,-1.8045] do not contain the number zero.


### Exercise 4

**Point 1**

**Point 2**

**Point 3**

**Point 4**

### Exercise 5


```{r}
data = read.table("nauseatable.txt", header=TRUE)
attach(data)

chlorpromazine_no_nausea = data[1,1]
chlorpromazine_nausea = data[1,2]
pentobarbital_100mg_no_nausea = data[2,1]
pentobarbital_100mg_nausea = data[2,2]
pentobarbital_150mg_no_nausea = data[3,1]
pentobarbital_150mg_nausea = data[3,2]
```


**Point 1**


The data given is re-arranged and the result is a dataframe which contains 304 rows and two columns. One column showing whether the patient suffered nausea and the other column showing which type of medicine was used. The data was re-arranged in the following way:


```{r}
nausea = c()
medicine = c()

for(i in 1:304){
  if(i <= 100){
    nausea[i] = 0
    medicine[i] = "chlorpromazine"
  }else if(i > 100 && i <= 152){
    nausea[i] = 1
    medicine[i] = "chlorpromazine"
  }else if(i > 152 && i <= 184){
    nausea[i] = 0
    medicine[i] = "pentobarbital 100mg"
  }else if(i > 184 && i <= 219){
    nausea[i] = 1
    medicine[i] = "pentobarbital 100mg"
  }else if(i > 219 && i <= 267){
    nausea[i]=0
    medicine[i] = "pentobarbital 150mg"
  }else if(i > 267 && i <= 304){
    nausea[i] = 1
    medicine[i] = "pentobarbital 150mg"
  }
}

nausea.frame = data.frame(nausea,medicine)
```


**Point 2**


Running the following code a contingency table is created and we can see that the values for nausea and medicine match the original data table. 


```{r}
xtabs(~medicine+nausea)
```

**Point 3**

In order to test whether the different medications work equally against nausea a permutation test was performed.


```{r}
attach(nausea.frame)

B=1000
tstar=numeric(B)
for(i in 1:B){
  treatstar=sample(medicine)
  tstar[i] = chisq.test(xtabs(~treatstar+nausea))[[1]]
}

myt=chisq.test(xtabs(~medicine+nausea))[[1]]

pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
pl
```

Looking at the value 'pl' we can see that it is > 0.05, therefore we accept the null hypothesis that the different medicines work equally against nausea.



**Point 4**


```{r}
pl;chisq.test(xtabs(~nausea+medicine))[[3]]
```

Looking at the p-value from the permutation test and from the chisquare test, we can clearly see that they are different. The Chi-squared test tests for the level of independence. The variable "myt" is based on the original values of medicine and nausea, while in the permutation test we change the sample for medicine in each iteration. This could be the reason for the difference in P-values.

### Exercise 6

**Point 1**

**Point 2**

**Point 3**

**Point 4**

**Point 5**

### Exercise 7
