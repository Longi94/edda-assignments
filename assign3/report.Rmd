---
title: "Assignment 3"
author: "Daniel Galea, Konrad Karas and Long Tran, group 6"
date: "14 March 2018"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
library(multcomp)
library(lme4)
```

## Exercises

### Exercise 1

Investigating our dataset (bread.txt) we can distinguish one numerical outcome (hours) and two factors that can be fixed at discrete number of levels (3 levels of environment [cold,intermediate,warm] and 2 levels of humidity [dry,wet]).

```{r}
data = read.table("bread.txt", header=TRUE)
data
```

Such a dataset is a good candidate to be tested by 2-way ANOVA method.

**Point 1**

Data randomization for 2-way ANOVA:

```{r}
N = 3; I = 3; J = 2
rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J)))
```

As a result we obtained 18 experimental units (18 slices of bread) with assigned levels of two factors.

* First row describes levels of environment factor.
* Second row describes levels of humidity factor.
* Third row describes unit index.

**Point 2**

```{r, echo = FALSE}
par(mfrow=c(1,2))
boxplot(hours~environment,data=data)
boxplot(hours~humidity,data=data)
```

Looking at mean values in boxplots, we can see that for intermediate and warm environments, bread slices decay much faster.
Similar observation corresponds to humidity: wet bread slices decay faster then dry ones.

```{r, echo = FALSE}
attach(data)
interaction.plot(environment,humidity,hours)
interaction.plot(humidity,environment,hours)
```

Interaction plots show that there is significant correlation between environment and humidity factors. Presented lines are not parallel.
Bread slices decay faster when they are wet and are kept in intermediate or warm environment.

**Point 3**

Analysis of variance:
To test the variance we decided to use 2-way ANOVA as we have one numerical outcome (hours) and two factors with fixed levels (humidity and environment).
As we assume interactions between factors, we use full model.

```{r}
data$environment = as.factor(data$environment)
data$humidity = as.factor(data$humidity)
dataaov = lm(hours~environment*humidity, data=data)
anova(dataaov)
```

Analysis of p-values:

* environment factor have a main effect (p-value = 2.461e-10 < 0.05) 
* humidity factor have a main effect (p-value = 4.316e-06 < 0.05)
* there is significat interaction between environment and humidity (p-value = 3.705e-07 < 0.05)

Regarding plots and 2-way ANOVA results, we can assume that both humidity and environment have influence on bread decay. The higher the temperature of the environment is and the higher the humidity is, the faster bread slices decay.

*ESTIMATION*

```{r}
contrasts(data$environment)=contr.sum
contrasts(data$humidity)=contr.sum
dataaov2=lm(hours~environment*humidity,data=data)
summary(dataaov2)
```


The p-values are for testing the null hypothesis that the coefficient is 0.

All obtained p-values are below 0.05 so we can assume that every coefficient has influence on result.


*95% confidence intervals*

```{r}
confint(dataaov2)
```



**Point 4**

Both factors, environment and humidity, interact with each other.
We are not able to test only one factor without second factor taken into account.
We cannot judge which factor has the greatest influence on the decay since factors interact with each other.

**Point 5**

Analysis of residuals:

```{r}
qqnorm(residuals(dataaov2))
qqline(residuals(dataaov2))
```

We can distinguish two outliers from trend line, but the rest seem to keep normal distribution.


```{r}
plot(fitted(dataaov2),residuals(dataaov2))
```

The residuals seem to be spread evenly.


### Exercise 2

```{r}
search = read.table("search.txt", header = TRUE)
attach(search)
```

**Point 1**

We can randomly assign each student to an interface using the following code.

```{r}
B = 5 # 5 user skill levels
I = 3 # 3 interface types

blocks = matrix(0, B, I)

for (i in 1:B) {
  blocks[i, ] = sample(((i - 1) * I + 1):(i * I))
}

kable(x = blocks, caption = "Randomization of students to interfaces")
```

The numbers in the table are the numbers assigned to each student and each row represents the type of student. The first row contains the student with skill level 1, the second row with skill level 2 and so on. In each row the first student will be assigned to interface 1, the second to interface 2 and the third to interface 3.

**Point 2**

We plot boxplots and interaction graphs.

```{r}
par(mfrow = c(1, 2))

boxplot(time ~ interface)
boxplot(time ~ skill)

interaction.plot(interface, skill, time)
interaction.plot(skill, interface, time)
```

On the first interaction graph a lot of lines are parallel, whil one the second one a lot of lies are nonparallel. We observe more parallel lines than nonparallel ones so we can conclude that there is no significatnt interaction between skill and interface.

**Point 3**

Since the levels of the factors are coded in numbers, we convert them to factors first.

```{r}
search$interface = as.factor(interface)
search$skill = as.factor(skill)
```

We do a 2-way ANOVA test to test the null hypothesis that the search time is the same for all three interfaces. Since in Point 2 we concluded that there are no interaction between skill and interface we do the test on the additive model.

```{r}
aovser = lm(time ~ interface + skill, data = search)
anova(aovser)
```

Both p-values are lower than 0.05, which means that both factors have a significant affect on the model and the null hypothesis can be rejected. The search time is not the same for all interfaces.

**Point 4**

We need to overrule the default contrasts first and run lm again.

```{r}
contrasts(search$interface) = contr.sum
contrasts(search$skill) = contr.sum
aovser2 = lm(time ~ interface + skill, data = search)
summary(aovser2)
estimate = 20.5467 + 2.1533 - (0.3133 - 2.3867)
estimate
```

The estimated time is **24.7734**.

**Point 5**

```{r}
par(mfrow = c(1, 2))
qqnorm(residuals(aovser))
qqline(residuals(aovser))
plot(fitted(aovser), residuals(aovser))
```

The Q-Q plot indicates a normal distribution and the residuals are distributed evenly. The ANOVA test assumed a normal distribution so these diagnostic plots back the results of the test.

**Point 6**

```{r}
friedman.test(time, interface, skill)
```

We got a p-value of **0.04076** so the null hypothesis that the interface has no effect on the search time can be rejected.

**Point 7**

We do a 1-way ANOVA test.

```{r}
aovinter = lm(time ~ interface, data = search)
anova(aovinter)
```

We got a p-value of **0.09642** so we cannot reject the null hypothesis, that the search time is the same for all interfaces. In the interaction graphs in Point 2 we obsorve that skill as a high impact on the outcome measurement. This analysis is not right to perform since we are ignoring the `skill` variable.

The ANOVA test assumes normal distribution and we can verify this with a Q-Q plot. THe plot below maybe curving slightly, but base don this plot it is still reasonable to assume normal distribution.

```{r}
qqnorm(residuals(aovinter))
qqline(residuals(aovinter))
```

### Exercise 3
```{r}
data = read.table("cream.txt", header=TRUE)
data$starter = factor(data$starter)
data$batch = factor(data$batch)
data$position = factor(data$position)
attach(data)
```


**Point 1**


A three-way test was performed in the following way:
```{r}
model1 = lm(acidity~starter+batch+position,data)
model2 = update(model1, . ~ . - starter:batch:position) #remove interactions
data.aov = aov(model2,data)
summary(data.aov)
```

Looking at the p-values, we can conclude that the starter and batch do not have an effect on the overall acidity (not sure about this)

//Not sure what to write about the summary yet.


**Point 2**


In order to test the hypotheses for all $$H0:\alpha i = \alpha i'$$ on equality of differences of the main effects for starter simultaneously, a multiple comparison test was performed.


```{r}
model = lm(acidity~starter+batch+position, data=data)
startermult = glht(model,linfct=mcp(starter="Tukey"))
summary(startermult)
```


Looking at the P values produced, we can see that $$ \alpha 4 = \alpha 1 $$,$$ \alpha 4 = \alpha 2 $$, $$ \alpha 4 = \alpha 3 $$, and $$ \alpha 5 = \alpha 4 $$ for the previously mentioned null hypothesis are rejected as they are  < 0.05


**Point 3**
//Not sure about this part. can't fid P value from point 1 summary that matches alpha2 = alpha1

**Point 4**
In order to find the confidence intervals for testing all differences of the main effect of starter we could use the "startermult" variable from the second question of this exercise. The test was performed in the following way:


```{r}
confint(startermult, level=0.95)
```
Looking at the result, we can conclude that the intervals [1.3201,4.2999], [1.4701,4.4499], [2.3001,5.2799], and [-4.7835,-1.8045] do not contain the number zero.


### Exercise 4

```{r}
cows = read.table("cow.txt", header = TRUE)
attach(cows)
```

**Point 1**

We investigate the linear model to see if treatmens have a significant effect or not.

```{r}
cows$id = factor(cows$id)
cows$per = factor(cows$per)
cowaov = lm(milk ~ treatment + id + per, data = cows)
summary(cowaov)
```

We can see the that p-value for `treatmentB` is **0.516536**, which means that we cannot reject the null hypothesis and the treatments do not have a significant effect on the result.

**Point 2**

The estimation can be read from the summary in the previous point. The slope of treatmentB is **-0.51** and the slope of treatmentA is **0.51**. Therefore the estimated difference in milk prudiction is **1.02**.

**Point 3**

We use the crossover design to perform the test with mixed effects.

```{r}
cows$id = factor(id)
cows$per = factor(per)
cowslmer = lmer(milk ~ treatment + order + per + (1 | id),
                data = cows, REML = FALSE)
cowslmer1 = lmer(milk ~ order + per + (1 | id), data = cows, REML = FALSE)
anova(cowslmer1, cowslmer)
```

Here we get a p-value of **0.446** so the null hypthesis cannot be rejected again. This is the same result as the one we got in Point 1 and the p-values are also close.

**Point 4**

The test returned a p-value of **0.8281**, which is compatible with the other results in that we also cannot reject the null hypothesis. However, a two sample paired t-test is invalid in this case because it only considers the difference of the means. It does not take other effects into account so the results cannot be trusted.

### Exercise 5


```{r}
data = read.table("nauseatable.txt", header=TRUE)
attach(data)

chlorpromazine_no_nausea = data[1,1]
chlorpromazine_nausea = data[1,2]
pentobarbital_100mg_no_nausea = data[2,1]
pentobarbital_100mg_nausea = data[2,2]
pentobarbital_150mg_no_nausea = data[3,1]
pentobarbital_150mg_nausea = data[3,2]
```


**Point 1**


The data given is re-arranged and the result is a dataframe which contains 304 rows and two columns. One column showing whether the patient suffered nausea and the other column showing which type of medicine was used. The data was re-arranged in the following way:


```{r}
nausea = c()
medicine = c()

for(i in 1:304){
  if(i <= 100){
    nausea[i] = 0
    medicine[i] = "chlorpromazine"
  }else if(i > 100 && i <= 152){
    nausea[i] = 1
    medicine[i] = "chlorpromazine"
  }else if(i > 152 && i <= 184){
    nausea[i] = 0
    medicine[i] = "pentobarbital 100mg"
  }else if(i > 184 && i <= 219){
    nausea[i] = 1
    medicine[i] = "pentobarbital 100mg"
  }else if(i > 219 && i <= 267){
    nausea[i]=0
    medicine[i] = "pentobarbital 150mg"
  }else if(i > 267 && i <= 304){
    nausea[i] = 1
    medicine[i] = "pentobarbital 150mg"
  }
}

nausea.frame = data.frame(nausea,medicine)
```


**Point 2**


Running the following code a contingency table is created and we can see that the values for nausea and medicine match the original data table. 


```{r}
xtabs(~medicine+nausea)
```

**Point 3**

In order to test whether the different medications work equally against nausea a permutation test was performed.


```{r}
attach(nausea.frame)

B=1000
tstar=numeric(B)
for(i in 1:B){
  treatstar=sample(medicine)
  tstar[i] = chisq.test(xtabs(~treatstar+nausea))[[1]]
}

myt=chisq.test(xtabs(~medicine+nausea))[[1]]

pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
pl
```

Looking at the value 'pl' we can see that it is > 0.05, therefore we accept the null hypothesis that the different medicines work equally against nausea.



**Point 4**


```{r}
pl;chisq.test(xtabs(~nausea+medicine))[[3]]
```

Looking at the p-value from the permutation test and from the chisquare test, we can clearly see that they are different. The Chi-squared test tests for the level of independence. The variable "myt" is based on the original values of medicine and nausea, while in the permutation test we change the sample for medicine in each iteration. This could be the reason for the difference in P-values.

### Exercise 6

```{r, echo = FALSE}
data = read.table("airpollution.txt", header=TRUE)
```

**Point 1**

```{r}
pairs(data, panel=panel.smooth)
```

In scatter plots we can see some factors that are highly correlated with oxidant that is a main factor of our investigation. Some slopes are highly positive (oxidant~temperature) or negative (oxidant~wind) so linear regression model should be useful here.

**Point 2**

*STEP 1*

We are setting a simple linear regression model for each of explanatory variable.

*Wind:*
```{r}
windlm=lm(oxidant~wind,data=data)
summary(windlm)
plot(oxidant~wind, data=data)
abline(windlm)
```

For *wind* variable we received R^2 equal to 0.5863 and p-value 8.20e-07.

The slope of plot is negative. We can assume significant correlation between wind and oxidant.


*Temperature:*
```{r}
templm=lm(oxidant~temperature,data=data)
summary(templm)
plot(oxidant~temperature, data=data)
abline(templm)
```

For *temperature* variable we received R^2 equal to 0.576 and p-value XXXX.

The slope of plot is positive. We can assume significant correlation between temperature and oxidant.


*Humidity:*
```{r}
humilm=lm(oxidant~humidity,data=data)
summary(humilm)
plot(oxidant~humidity, data=data)
abline(humilm)
```

For *humidity* variable we received R^2 equal to 0.124 and p-value XXXX.

Low R^2 and high p-value (>0.05) signify that there in no significant correlation between humidity and oxidant. What's more, there is no clear linear relation between X  and Y


*Insolation:*
```{r}
insolm=lm(oxidant~insolation,data=data)
summary(insolm)
plot(oxidant~insolation, data=data)
abline(insolm)
```

For *insolation* variable we received R^2 equal to 0.0.22993 and p-value XXXX.

The slope of plot is positive. The correlation between insolation and oxidant is not so significant as we can see outlying points in X and no clear linear relation between X an Y.


*Result*

Considering all received R^2 values, for further processing, we should select *wind* for an explanatory variable X_j as it has the highest value of that coefficient.


*STEP 2*

Now, we are going to extend our current model (oxidant~wind) by another explanatory variable.


*Temperature*
```{r}
windTemplm=lm(oxidant~wind+temperature,data=data)
summary(windTemplm)
```
After extending our current model by *temperature* variable, R^2 value increased to 0,7773.

*Humidity*
```{r}
windHumilm=lm(oxidant~wind+humidity,data=data)
summary(windHumilm)
```
After extending our current model by *humidity* variable, R^2 value insignificantly increased to 0,5913.

*Insolation*
```{r}
windInsolm=lm(oxidant~wind+insolation,data=data)
summary(windInsolm)
```
After extending our current model by *insolation* variable, R^2 value increased to 0,6613.


*Result*

For further processing we should extend our model by *temperature* variable as this variable caused the biggest increase in R^2 value.


*STEP 3*

In this step we proceed with extending our current model (oxidant~wind+temperature - R^2 = 0,7773).


*Humidity*
```{r}
windTempHumilm=lm(oxidant~wind+temperature+humidity,data=data)
summary(windTempHumilm)
```
After extending our current model by *humidity* variable, R^2 value increased to 0,7964.

*Insolation*
```{r}
windTempInslm=lm(oxidant~wind+temperature+insolation,data=data)
summary(windTempInslm)
```
After extending our current model by *insolation* variable, R^2 value increased to 0,7816.

Neither *humidity* nor *insolation* increased significaly R^2 value.
In such a situation we should stop extending our model and assume that model from previous step is sufficient (oxidant~wind+tempreature).

Resulting linear regression model: 

$$total = -5.20334 +-0.42706*wind - 0.52035*temperature + error$$

**Point 3**

All variables included in model:

```{r}
allVarlm=lm(oxidant~wind+temperature+insolation+humidity,data=data)
summary(allVarlm)
```

Regarding the summary we can distinguish that *insolation* variable has the highest p-value (0.65728) that is greather than 0.05. We should exclude that variable from our model.


```{r}
windTempHumilm=lm(oxidant~wind+temperature+humidity,data=data)
summary(windTempHumilm)
```

In next iteration we can see that also *humidity* variable is not appropriate for our testing. It's p-value (0.131) is greather than 0.05. We should exclude that variable too.

```{r}
windTemplm=lm(oxidant~wind+temperature,data=data)
summary(windTemplm)
```

Now all variables are significant.

As a result, we obtained exactly the same model as received in previous point.

Resulting linear regression model: 

$$total = -5.20334 +-0.42706*wind - 0.52035*temperature + error$$

**Point 4**

THAT ONE?

Resulting linear regression model: 

$$total = -5.20334 +-0.42706*wind - 0.52035*temperature + error$$

**Point 5**

Investigation of residuals.

```{r}
qqnorm(residuals(windTemplm))
qqline(residuals(windTemplm))
```

Residuals represent normal distribution. We can assume that linear regression model was useful in that exercise.

### Exercise 7

```{r, echo = FALSE}
expensescrime = read.csv(file = "expensescrime.txt", header = TRUE, sep = " ")
attach(expensescrime)
```

**Step up**

First we use the step-up method to build a model that explains the `expend` parameter.

```{r}
summary(lm(expend ~ bad, data = expensescrime))
summary(lm(expend ~ crime, data = expensescrime))
summary(lm(expend ~ lawyers, data = expensescrime))
summary(lm(expend ~ employ, data = expensescrime))
summary(lm(expend ~ pop, data = expensescrime))
```

The `employ` variable has the highest p-value so wo add that to the model.

```{r}
summary(lm(expend ~ employ + bad, data = expensescrime))
summary(lm(expend ~ employ + crime, data = expensescrime))
summary(lm(expend ~ employ + lawyers, data = expensescrime))
summary(lm(expend ~ employ + pop, data = expensescrime))
```

The `lawyer` variable has the highest p-value so wo add that to the model.

```{r}
summary(lm(expend ~ employ + lawyers + bad, data = expensescrime))
summary(lm(expend ~ employ + lawyers + crime, data = expensescrime))
summary(lm(expend ~ employ + lawyers + pop, data = expensescrime))
```

All of the p-values are now higher than 0.05 so the rest of the variables do not have a significant impact on tht `expend` variable. The resulting model is following.

$$ expend = -1.107e2 + 2.686e{-2} * lawyers + 2.971e{-2} * employ + error $$

**Step down**

We build a model using the step down method as well to make sure that we get the best model possible that describes the `expend` variable.

```{r}
summary(lm(expend ~ employ + lawyers + bad + crime + pop, data = expensescrime))
```

The `crime` variable has the highest p-value so we removed that from the model.

```{r}
summary(lm(expend ~ employ + lawyers + bad + pop, data = expensescrime))
```

The `pop` variable has the highest p-value so we removed that from the model.

```{r}
summary(lm(expend ~ employ + lawyers + bad, data = expensescrime))
```

The `bad` variable has the highest p-value so we removed that from the model.

```{r}
summary(lm(expend ~ employ + lawyers, data = expensescrime))
```

All the p-values are now significant so we stop the step down loop. We got the same model as the one we got with the step up model.

```{r}
expendlm = lm(expend ~ employ + lawyers, data = expensescrime)
```

**Influence points**

To investigate the influence points we calculate the Cook's distances.

```{r}
round(cooks.distance(expendlm), 2)
```

All the distances are below 1 so there are no influence points.

**Collinearity**

To check for collinearity we draw scatter plots between the variables.

```{r}
pairs(expensescrime[,c(2, 5, 6)], panel=panel.smooth)
```

The variables `lawyers` and `employ` look very collinear. We also compute the correlation between the variables and the result of **0.97** supports the result of the plots.

```{r}
round(cor(expensescrime[,2:7]), 2)
```

**Residuals**

To investigate the residuals we draw a Q-Q plot and a scatter plot.

```{r}
par(mfrow = c(1, 2))
qqnorm(residuals(expendlm))
qqline(residuals(expendlm))
plot(fitted(expendlm), residuals(expendlm))
```

According to these plots we cannot assume normality and there are extreme values as well. The spread of the residuals is smaller for small values and lot higher for high values, the plot is not symmetrical at all. As a result, we cannot consider the model we got a good model.
